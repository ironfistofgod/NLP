{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ',1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        scores = {}\n",
    "        for word in self.word2vec.keys():\n",
    "            scores[word] = self.score(w, word)\n",
    "        return sorted(scores, key=scores.get, reverse=True)[1:K+1]\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        v1 = self.word2vec[w1]\n",
    "        v2 = self.word2vec[w2]\n",
    "        return np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0.7775108541288563\n",
      "germany berlin 0.7420295235998394\n",
      "['cats', 'kitty', 'kitten', 'feline', 'kitties']\n",
      "['dogs', 'puppy', 'Dog', 'doggie', 'canine']\n",
      "['dog', 'pooches', 'Dogs', 'doggies', 'canines']\n",
      "['france', 'Paris', 'london', 'berlin', 'tokyo']\n",
      "['austria', 'europe', 'german', 'berlin', 'poland']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'))\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                sentemb.append(np.mean([self.w2v.word2vec[w] for w in sent.split() if w in self.w2v.word2vec], axis=0))\n",
    "            else:\n",
    "                sentemb.append(np.sum([self.w2v.word2vec[w] * idf[w] for w in sent.split() if w in self.w2v.word2vec and w in idf], axis=0))\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        scores = {}\n",
    "        for sent in sentences:\n",
    "            scores[sent] = self.score(s, sent, idf)\n",
    "        return sorted(scores, key=scores.get, reverse=True)[1:K+1]\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        v1 = self.encode([s1], idf)[0]\n",
    "        v2 = self.encode([s2], idf)[0]\n",
    "        return np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            for w in set(sent.split()):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        for word in idf:\n",
    "            idf[word] = max(1, np.log10(len(sentences) / (idf[word])))    \n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "['an african american man smiling . ', 'a little african american boy and girl looking up . ', 'an afican american woman standing behind two small african american children . ', 'an african american man is sitting . ', 'a girl in black hat holding an african american baby . ']\n",
      "0.5726258859719605\n",
      "['an african american man smiling . ', 'an african american man is sitting . ', 'a little african american boy and girl looking up . ', 'an afican american woman standing behind two small african american children . ', 'a girl in black hat holding an african american baby . ']\n",
      "0.4751450875368783\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'))\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "sentences = open(os.path.join(PATH_TO_DATA, 'sentences.txt')).read().splitlines() \n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print(s2v.most_similar('' if not sentences else sentences[10], sentences))  # BoV-mean\n",
    "print(s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13]))\n",
    "\n",
    "\n",
    "print(s2v.most_similar('' if not sentences else sentences[10], sentences, idf))  # BoV-idf\n",
    "print(s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "\n",
    "eng = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax = 50000)\n",
    "fra = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "en_vocab = set(eng.word2id.keys())\n",
    "fr_vocab = set(fra.word2id.keys())\n",
    "common_vocab = list(en_vocab & fr_vocab)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for word in common_vocab:\n",
    "    X.append(fra.word2vec[word])\n",
    "    Y.append(eng.word2vec[word])\n",
    "X = np.matrix(X)\n",
    "Y = np.matrix(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "U, s, V_t = np.linalg.svd(np.dot(X.transpose(), Y))\n",
    "W = np.dot(U, V_t)\n",
    "fr_aligned_vectors = np.dot(list(fra.word2vec.values()), W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flower', 'flowers', 'rosette', 'flowered', 'petals']\n",
      "['tree', 'trees', 'understory', 'seedlings', 'shrubs']\n",
      "['café', 'coffee', 'cafe', 'cafés', 'chocolates']\n",
      "['paris', 'parisian', 'rouen', 'gallimard', 'sorbonne']\n",
      "['germany', 'rhineland', 'gelsenkirchen', 'saarland', 'bavaria']\n",
      "['cat', 'chat', 'dog', 'chats', 'chien']\n",
      "['bouteille', 'bouteilles', 'flacon', 'bottle', 'vodka']\n",
      "['voiture', 'voitures', 'automobile', 'porsche', 'automobiles']\n",
      "['hat', 'chapeau', 'cowboy', 'shirt', 'trick']\n",
      "['paris', 'parisienne', 'lyon', 'versailles', 'paris,']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "fr_aligned = dict()\n",
    "fr_wordlist = list(fra.word2vec.keys())\n",
    "for i in range(len(fr_aligned_vectors)):\n",
    "    fr_aligned[fr_wordlist[i]] = fr_aligned_vectors[i]\n",
    "    \n",
    "for w in ['fleur', 'arbre', 'café', 'paris', 'allemagne']:\n",
    "    scores = {}\n",
    "    for word in eng.word2vec.keys():\n",
    "        v1 = fr_aligned[w]\n",
    "        v2 = eng.word2vec[word]\n",
    "        scores[word] = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    print(sorted(scores, key=scores.get, reverse=True)[0:5])  \n",
    "    \n",
    "for w in ['cat', 'bottle', 'car', 'hat', 'paris']:\n",
    "    scores = {}\n",
    "    for word in fr_aligned.keys():\n",
    "        v1 = eng.word2vec[w]\n",
    "        v2 = np.array(fr_aligned[word])[0]\n",
    "        scores[word] = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    print(sorted(scores, key=scores.get, reverse=True)[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 pretrained sentences from train set\n",
      "Loaded 1101 pretrained sentences from dev set\n",
      "Loaded 2210 pretrained sentences from test set\n"
     ]
    }
   ],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "def load_train_dev(fname, name=''):\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "\n",
    "        score=[]\n",
    "        sentences=[]\n",
    "        for sent in f:\n",
    "            score.append(int(sent.split(' ', 1)[0]))\n",
    "            sentences.append(sent.split(' ', 1)[1])\n",
    "        #score = [sent.split(' ', 1)[0] for sent in f]\n",
    "        #sentences = [sent.split(' ', 1)[1] for sent in f]\n",
    "\n",
    "    print('Loaded %s pretrained sentences from %s set' % (len(sentences), name))\n",
    "    return [x.strip() for x in sentences], np.array(score)\n",
    "\n",
    "def load_test(fname, name=''):\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        \n",
    "        sentences = [sent for sent in f]\n",
    "        \n",
    "    print('Loaded %s pretrained sentences from %s set' % (len(sentences), name))\n",
    "    return [x.strip() for x in sentences]\n",
    "    \n",
    "train_x, train_y = load_train_dev(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'), 'train')\n",
    "dev_x, dev_y = load_train_dev(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'), 'dev')\n",
    "test_x = load_test(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 190000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=190000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "train_xvec = np.array(s2v.encode(train_x))\n",
    "dev_xvec = np.array(s2v.encode(dev_x))\n",
    "test_xvec = np.array(s2v.encode(test_x))\n",
    "\n",
    "\n",
    "idf = s2v.build_idf(train_x+dev_x)\n",
    "\n",
    "\n",
    "train_xidf = np.array(s2v.encode(train_x, idf))\n",
    "dev_xidf = np.array(s2v.encode(dev_x, idf))\n",
    "test_xidf = np.array(s2v.encode(test_x, idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139   0   0   0   0]\n",
      " [  0 289   0   0   0]\n",
      " [  0   0 229   0   0]\n",
      " [  0   0   0 279   0]\n",
      " [  0   0   0   0 165]]\n",
      "\n",
      "\n",
      "Average of word vectors : \n",
      "LogisticRegression(C=0.16, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Train error :  0.46512172284644193\n",
      "Dev error :  0.4332425068119891\n",
      "Confusion matrix :\n",
      "[[  9   5   2   2   0]\n",
      " [109 203  98  50  11]\n",
      " [  2   6  14   3   1]\n",
      " [ 17  75 110 210 112]\n",
      " [  2   0   5  14  41]]\n",
      "\n",
      "\n",
      "Weighted average of word vectors : \n",
      "LogisticRegression(C=0.14, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Train error :  0.5010533707865169\n",
      "Dev error :  0.4032697547683924\n",
      "Confusion matrix :\n",
      "[[ 32  29  16   5   2]\n",
      " [ 74 154  72  41   7]\n",
      " [ 14  31  40  34   1]\n",
      " [ 12  64  84 138  75]\n",
      " [  7  11  17  61  80]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Function to create a logistic regression model. Return the fitted model\n",
    "def logReg(data, model_lr=LogisticRegression()):\n",
    "    train_x = data[0]\n",
    "    train_y = data[1]\n",
    "    dev_x = data[2]\n",
    "    dev_y = data[3]\n",
    "    model_lr.fit(train_x, train_y) # Fit the model\n",
    "    pred = model_lr.predict(dev_x) # Predict dev\n",
    "    print(model)\n",
    "    print('Train error : ', model_lr.score(train_x, train_y)) # Print score\n",
    "    print('Dev error : ', accuracy_score(pred, dev_y))\n",
    "    print('Confusion matrix :')\n",
    "    print(confusion_matrix(pred, dev_y))\n",
    "    print('\\n')\n",
    "    return model_lr\n",
    "\n",
    "''' After a grid_search on weighted/average, C & penalty, \n",
    " the best score on train/dev is reach with idf_average, l2 and C=0.14 '''\n",
    "\n",
    "# Selection of weighted dataset\n",
    "average_bov = [train_xvec, train_y, dev_xvec, dev_y]\n",
    "weighted_bov = [train_xidf, train_y, dev_xidf, dev_y]\n",
    "\n",
    "# Create and fit the model with optimized parameters, and print some metrics\n",
    "print(confusion_matrix(dev_y, dev_y))\n",
    "print('\\n')\n",
    "\n",
    "print('Average of word vectors : ')\n",
    "model = LogisticRegression(penalty='l2', C=0.16)\n",
    "model = logReg(average_bov, model)\n",
    "\n",
    "print('Weighted average of word vectors : ')\n",
    "model = LogisticRegression(penalty='l2', C=0.14)\n",
    "model = logReg(weighted_bov, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "test_y = model.predict(test_xidf)\n",
    "\n",
    "# Saving results\n",
    "file = open('data/logreg_bov_y_test_sst.txt', 'w')\n",
    "for i in test_y:\n",
    "    file.write(str(i))\n",
    "    file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Score on train :  0.41947565543071164\n",
      "Score on dev :  0.35603996366939145\n",
      "[[ 28  18  12   4   1]\n",
      " [ 49 118  50  28   6]\n",
      " [ 14  40  29  32   3]\n",
      " [ 40 102 118 155  93]\n",
      " [  8  11  20  60  62]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "def fitting_other_classifier(data, model):\n",
    "    train_x = data[0]\n",
    "    train_y = data[1]\n",
    "    dev_x = data[2]\n",
    "    dev_y = data[3]\n",
    "    model.fit(train_x, train_y) # Fit the model\n",
    "    pred = model.predict(dev_x) # Predict dev\n",
    "    print(model)\n",
    "    print('Score on train : ', model.score(train_x, train_y)) # Print score\n",
    "    print('Score on dev : ', accuracy_score(pred, dev_y))\n",
    "    print(confusion_matrix(pred, dev_y))\n",
    "    print('\\n')\n",
    "    # Saving results\n",
    "    file = open('data/linearsvc_bov_y_test_sst.txt', 'w')\n",
    "    for i in pred:\n",
    "        file.write(str(i))\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return model\n",
    "\n",
    "weighted_bov = [train_xidf, train_y, dev_xidf, dev_y]\n",
    "\n",
    "# Create and fit the model with optimized parameters, and print some metrics\n",
    "model = LinearSVC()\n",
    "model = fitting_other_classifier(weighted_bov, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 pretrained sentences from train set\n",
      "Loaded 1101 pretrained sentences from dev set\n",
      "Loaded 2210 pretrained sentences from test set\n"
     ]
    }
   ],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "\n",
    "def load_train_dev(fname, name=''):\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "\n",
    "        score=[]\n",
    "        sentences=[]\n",
    "        for sent in f:\n",
    "            score.append(int(sent.split(' ', 1)[0]))\n",
    "            sentences.append(sent.split(' ', 1)[1])\n",
    "\n",
    "    print('Loaded %s pretrained sentences from %s set' % (len(sentences), name))\n",
    "    return [x.strip() for x in sentences], np.array(score)\n",
    "\n",
    "def load_test(fname, name=''):\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        \n",
    "        sentences = [sent for sent in f]\n",
    "        \n",
    "    print('Loaded %s pretrained sentences from %s set' % (len(sentences), name))\n",
    "    return [x.strip() for x in sentences]\n",
    "\n",
    "def get_dummies(data): # transform np.array into dummy variable to fit models\n",
    "    result = np.array([[0 for i in range(data.min(), data.max()+1)] for i in data])\n",
    "    for i, k in enumerate(data):\n",
    "        result[i][k] = 1\n",
    "    return result\n",
    "    \n",
    "train_x, train_y = load_train_dev(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'), 'train')\n",
    "dev_x, dev_y = load_train_dev(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'), 'dev')\n",
    "test_x = load_test(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), 'test')\n",
    "\n",
    "train_y = get_dummies(train_y)\n",
    "dev_y = get_dummies(dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "245344"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "from keras.preprocessing import text\n",
    "\n",
    "filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "train_all_voc = [word for sent in train_x for word in sent.split(' ')]\n",
    "\n",
    "\n",
    "train_voc_size = round(len([word for sent in train_x for word in sent.split(' ')])*1.5)\n",
    "\n",
    "train_int = [text.one_hot(text=sent, n=train_voc_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \") \n",
    "             for sent in train_x]\n",
    "dev_int = [text.one_hot(text=sent, n=train_voc_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \") \n",
    "           for sent in dev_x]\n",
    "test_int = [text.one_hot(text=sent, n=train_voc_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \") \n",
    "            for sent in test_x]\n",
    "\n",
    "train_voc_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_len = len(max(train_int, key=len))+20\n",
    "\n",
    "train_int = sequence.pad_sequences(train_int, maxlen=max_len)\n",
    "dev_int = sequence.pad_sequences(dev_int, maxlen=max_len)\n",
    "test_int = sequence.pad_sequences(test_int, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 300  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = train_voc_size  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         73603200  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 73,696,965\n",
      "Trainable params: 73,696,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'rmsprop' #'rmsprop' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/3\n",
      "8544/8544 [==============================] - 258s 30ms/step - loss: 1.5571 - acc: 0.2940 - val_loss: 1.4815 - val_acc: 0.3379\n",
      "Epoch 2/3\n",
      "8544/8544 [==============================] - 221s 26ms/step - loss: 1.3490 - acc: 0.4086 - val_loss: 1.3630 - val_acc: 0.3797\n",
      "Epoch 3/3\n",
      "8544/8544 [==============================] - 219s 26ms/step - loss: 1.1446 - acc: 0.4603 - val_loss: 1.3883 - val_acc: 0.3815\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAEKCAYAAACSdBVoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1182d9ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x162e50f98>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "bs = 64 \n",
    "n_epochs = 3\n",
    "\n",
    "x_train = np.array(train_int)\n",
    "y_train = train_y\n",
    "x_val = np.array(dev_int)\n",
    "y_val = dev_y\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(x_val, y_val))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(history.history['acc'], c='blue', alpha=0.8, label='Train accuracy')\n",
    "plt.plot(history.history['val_acc'], c='green', alpha=0.8, label='Dev accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks([i for i in range(n_epochs)], [i+1 for i in range(n_epochs)])\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "pred_test = model.predict_classes(test_int)\n",
    "\n",
    "# Saving results\n",
    "file = open('data/logreg_lstm_y_test_sst.txt', 'w')\n",
    "for l in pred_test:\n",
    "    file.write(str(l))\n",
    "    file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 69, 300)           73603200  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 62, 32)            76832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 31, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 992)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 992)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                49650     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 73,729,937\n",
      "Trainable params: 73,729,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten, Dropout, TimeDistributed\n",
    "\n",
    "input_len = len(max(train_int, key=len))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, embed_dim, input_length=input_len))\n",
    "model2.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(50, activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(n_classes, activation='sigmoid'))\n",
    "print(model2.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/3\n",
      "8544/8544 [==============================] - 248s 29ms/step - loss: 1.5728 - acc: 0.2699 - val_loss: 1.5489 - val_acc: 0.3406\n",
      "Epoch 2/3\n",
      "8544/8544 [==============================] - 3492s 409ms/step - loss: 1.4044 - acc: 0.3885 - val_loss: 1.3720 - val_acc: 0.3924\n",
      "Epoch 3/3\n",
      "8544/8544 [==============================] - 233s 27ms/step - loss: 1.1162 - acc: 0.4945 - val_loss: 1.4148 - val_acc: 0.3878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x113627048>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "model2.fit(x_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(x_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_prediction = model2.predict(test_int, batch_size=bs).argmax(axis=-1)\n",
    "model_prediction.tofile('conv_y_test_sst.txt', sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
